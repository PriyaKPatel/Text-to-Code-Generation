# TEXTâ€‘TOâ€‘CODE GENERATION PLATFORM
## Unified Technical Documentation

This document is a **cleaned, deduplicated, productionâ€‘ready documentation**
compiled from all project guides, phase summaries, and workflows.

---
# ðŸš€ PHASE 2: AWS SETUP - DETAILED STEP-BY-STEP GUIDE


---

## ðŸ“‹ Overview

This guide will walk you through setting up all AWS resources needed to deploy your text-to-code application. Estimated time: **1-2 hours**.

**What you'll create:**

1. IAM User with programmatic access
2. ECR Repository (for Docker images)
3. S3 Bucket (for model storage)
4. EC2 Instance (to run your application)

---


---

## STEP 2.1: AWS Account & IAM User Setup


---

### 1.1 Login to AWS Console

1. Go to: https://aws.amazon.com/
2. Click **"Sign In to the Console"** (top right)
3. Enter your AWS account email and password
4. If you don't have an account:
   - Click **"Create an AWS Account"**
   - Follow the registration process
   - **Note**: You'll need a credit card, but free tier covers most costs for 12 months


---

### 1.2 Navigate to IAM

1. Once logged in, in the search bar at the top, type: **"IAM"**
2. Click on **"IAM"** service
3. You should see the IAM Dashboard


---

### 1.3 Create IAM User

**Step-by-step:**

1. **Click "Users"** in the left sidebar (under "Access management")
2. **Click "Create user"** button (top right)
3. **User details:**

   - **User name**: `text-to-code-deploy`
   - **Provide user access to the AWS Management Console**: Leave **UNCHECKED** (we only need programmatic access)
   - Click **"Next"**
4. **Set permissions:**

   - Select: **"Attach policies directly"**
   - Search and check these policies (one by one):
     - âœ… `AmazonEC2FullAccess`
     - âœ… `AmazonS3FullAccess`
     - âœ… `AmazonECRFullAccess`
   - Click **"Next"**
5. **Review and create:**

   - Review the user name and policies
   - Click **"Create user"**
6. **âš ï¸ CRITICAL: Save Access Keys**

   - You'll see a success message
   - Click **"Create access key"** button
   - Select: **"Command Line Interface (CLI)"**
   - Check the confirmation box
   - Click **"Next"**
   - **Description** (optional): `Text-to-Code Deployment`
   - Click **"Create access key"**
7. **âš ï¸ DOWNLOAD/SAVE THESE IMMEDIATELY:**

   ```
   Access Key ID: AKIAXXXXXXXXXXXXXXXX
   Secret Access Key: xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
   ```

   - Click **"Download .csv file"** and save it securely
   - **OR** copy both values to a secure password manager
   - **âš ï¸ WARNING**: Secret key is shown ONLY ONCE. If you lose it, you'll need to create a new one.
8. Click **"Done"**


---

### 1.4 Install AWS CLI

**On macOS (your system):**

```bash

---

# Check if AWS CLI is already installed
aws --version


---

# If not installed, install using Homebrew
brew install awscli


---

# OR install using pip (in your venv)
source venv/bin/activate
pip install awscli
```

**Verify installation:**

```bash
aws --version

---

# Should show: aws-cli/2.x.x Python/3.x.x ...
```


---

### 1.5 Configure AWS CLI

```bash

---

# Configure AWS CLI
aws configure


---

# You'll be prompted for:

---

# AWS Access Key ID: [Paste your Access Key ID from step 1.3]

---

# AWS Secret Access Key: [Paste your Secret Access Key from step 1.3]

---

# Default region name: us-east-1

---

# Default output format: json
```

**Example:**

```
AWS Access Key ID [None]: AKIAIOSFODNN7EXAMPLE
AWS Secret Access Key [None]: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY
Default region name [None]: us-east-1
Default output format [None]: json
```


---

### 1.6 Verify AWS Configuration

```bash

---

# Test your configuration
aws sts get-caller-identity


---

# Expected output:

---

# {

---

#     "UserId": "AIDA...",

---

#     "Account": "123456789012",

---

#     "Arn": "arn:aws:iam::123456789012:user/text-to-code-deploy"

---

# }
```

**âœ… Save your Account ID** from the output - you'll need it later!

**If you get an error:**

- Double-check your access key and secret key
- Make sure there are no extra spaces when pasting
- Verify the IAM user has the correct policies attached

---


---

## STEP 2.2: Create ECR Repository


---

### 2.1 Create ECR Repository via CLI

```bash

---

# Create ECR repository
aws ecr create-repository \
    --repository-name text-to-code \
    --region us-east-1


---

#     "repository": {

---

#         "repositoryUri": "123456789012.dkr.ecr.us-east-1.amazonaws.com/text-to-code",

---

#         "registryId": "123456789012",

---

#         "repositoryName": "text-to-code",

---

#         ...

---

#     }

---

### 2.2 Verify ECR Repository

```bash

---

# List all ECR repositories
aws ecr describe-repositories --region us-east-1


---

# Should show your text-to-code repository
```


---

### 2.3 Alternative: Create via AWS Console

If you prefer the web interface:

1. Go to AWS Console â†’ Search **"ECR"** â†’ Click **"Elastic Container Registry"**
2. Click **"Create repository"**
3. **Repository settings:**
   - **Visibility**: Private
   - **Repository name**: `text-to-code`
   - **Tag immutability**: Disabled (for now)
   - **Scan on push**: Enabled (recommended for security)
4. Click **"Create repository"**
5. Copy the **Repository URI** from the success page

---


---

## STEP 2.3: Create S3 Bucket


---

### 3.1 Create S3 Bucket

**âš ï¸ Important**: S3 bucket names must be **globally unique** across all AWS accounts.

```bash

---

# Replace 'priya' with your unique identifier (name, initials, etc.)
aws s3 mb s3://text-to-code-models-priya --region us-east-1


---

# make_bucket: text-to-code-models-priya
```

**If you get "BucketAlreadyExists" error:**

- Try a different name:
  ```bash
  aws s3 mb s3://text-to-code-models-priyakpatel --region us-east-1
  # OR
  aws s3 mb s3://text-to-code-models-priya-2024 --region us-east-1
  ```


---

### 3.2 Verify S3 Bucket

```bash

---

# List your S3 buckets
aws s3 ls


---

# Should show:

---

# text-to-code-models-priya
```


---

### 3.3 Alternative: Create via AWS Console

1. Go to AWS Console â†’ Search **"S3"** â†’ Click **"S3"**
2. Click **"Create bucket"**
3. **General configuration:**
   - **Bucket name**: `text-to-code-models-priya` (must be unique)
   - **AWS Region**: US East (N. Virginia) us-east-1
4. **Object Ownership**: ACLs disabled (recommended)
5. **Block Public Access**: Keep all settings enabled (for security)
6. **Bucket Versioning**: Disable (for now)
7. Click **"Create bucket"**

---


---

## STEP 2.4: Launch EC2 Instance


---

### 4.1 Navigate to EC2 Console

1. Go to AWS Console â†’ Search **"EC2"** â†’ Click **"EC2"**
2. You should see the EC2 Dashboard


---

### 4.2 Launch Instance

1. Click **"Launch Instance"** button (top right, orange button)


---

### 4.3 Configure Instance - Step 1: Name and Tags

- **Name**: `text-to-code-api`
- Click **"Next"**


---

### 4.4 Configure Instance - Step 2: Application and OS Images

- **AMI**: Select **"Ubuntu"** â†’ Choose **"Ubuntu Server 22.04 LTS"** (free tier eligible)
- **Architecture**: x86_64 (default)
- Click **"Next"**


---

### 4.5 Configure Instance - Step 3: Instance Type

- **Instance type**:
  - For **testing/free tier**: Select **"t2.micro"** (free tier eligible)
  - For **production**: Select **"t3.medium"** or **"t3.large"**
- Click **"Next"**


---

### 4.6 Configure Instance - Step 4: Key Pair

**âš ï¸ CRITICAL: Create and download key pair**

1. Click **"Create new key pair"**
2. **Key pair name**: `text-to-code-key`
3. **Key pair type**: RSA
4. **Private key file format**: `.pem` (for Mac/Linux)
5. Click **"Create key pair"**
6. **âš ï¸ The .pem file will download automatically - SAVE IT SECURELY!**
   - Move it to a safe location: `~/Downloads/text-to-code-key.pem`
   - Set proper permissions: `chmod 400 ~/Downloads/text-to-code-key.pem`
   - **You cannot download this again!**


---

### 4.7 Configure Instance - Step 5: Network Settings

1. Click **"Edit"** (top right of network settings section)
2. **Security group name**: `text-to-code-sg`
3. **Description**: `Security group for text-to-code API`
4. **Inbound security group rules** - Add these rules:

   **Rule 1: SSH**

   - **Type**: SSH
   - **Source type**: My IP (automatically fills your IP)
   - **Description**: Allow SSH from my IP

   **Rule 2: HTTP**

   - Click **"Add security group rule"**
   - **Type**: HTTP
   - **Source type**: Anywhere-IPv4 (0.0.0.0/0)
   - **Description**: Allow HTTP from anywhere

   **Rule 3: HTTPS**

   - Click **"Add security group rule"**
   - **Type**: HTTPS
   - **Source type**: Anywhere-IPv4 (0.0.0.0/0)
   - **Description**: Allow HTTPS from anywhere

   **Rule 4: Custom TCP**

   - Click **"Add security group rule"**
   - **Type**: Custom TCP
   - **Port range**: `8000`
   - **Source type**: Anywhere-IPv4 (0.0.0.0/0)
   - **Description**: Allow API port 8000
5. Click **"Next"**


---

### 4.8 Configure Instance - Step 6: Configure Storage

- **Volume size**: `30` GB (default is 8 GB, increase to 30)
- **Volume type**: gp3 (default)
- Click **"Next"**


---

### 4.9 Configure Instance - Step 7: Advanced Details

- Leave all defaults
- Click **"Next"**


---

### 4.10 Review and Launch

1. Review all settings:

   - âœ… Name: text-to-code-api
   - âœ… AMI: Ubuntu 22.04
   - âœ… Instance type: t2.micro (or your choice)
   - âœ… Key pair: text-to-code-key
   - âœ… Security group: text-to-code-sg with 4 rules
   - âœ… Storage: 30 GB
2. Click **"Launch Instance"**
3. You'll see: **"Successfully launched instance"**
4. Click **"View all instances"** (bottom right)


---

### 4.11 Get Instance Public IP

1. In the EC2 Instances list, find your instance: `text-to-code-api`
2. Wait for **"Instance state"** to show **"Running"** (takes 1-2 minutes)
3. Once running, check the **"Public IPv4 address"** column
4. **âœ… Copy the Public IP** (e.g., `54.123.45.67`)


---

### 4.12 Set Up Key Pair Permissions (on your Mac)

```bash

---

# Navigate to where you saved the .pem file
cd ~/Downloads


---

# Set proper permissions (required for SSH)
chmod 400 text-to-code-key.pem


---

# Test SSH connection (replace with your EC2 IP)
ssh -i text-to-code-key.pem ubuntu@YOUR_EC2_IP


---

# If successful, you'll see Ubuntu welcome message

---

# Type 'exit' to disconnect
```

**If SSH fails:**

- Wait 2-3 minutes after instance launch (SSH service needs to start)
- Verify security group allows SSH from your IP
- Check that you're using the correct key file path
- Verify the instance is in "Running" state

---


---

## âœ… STEP 2.5: Verification Checklist

Run these commands to verify everything is set up correctly:


---

### Verify IAM User

```bash
aws sts get-caller-identity

---

# Should show your IAM user: text-to-code-deploy
```


---

### Verify ECR Repository

```bash
aws ecr describe-repositories --region us-east-1 --repository-names text-to-code

---

# Should show your repository details
```


---

### Verify S3 Bucket

```bash
aws s3 ls | grep text-to-code

---

# Should show: text-to-code-models-priya
```


---

### Verify EC2 Instance

```bash
aws ec2 describe-instances \
    --filters "Name=tag:Name,Values=text-to-code-api" \
    --query "Reservations[*].Instances[*].[InstanceId,State.Name,PublicIpAddress]" \
    --output table

---

# Should show your instance as "running" with a public IP
```


---

### Test SSH Connection

```bash

---

# Replace with your actual EC2 IP
ssh -i ~/Downloads/text-to-code-key.pem ubuntu@YOUR_EC2_IP


---

# If successful, you're connected!

---

## ðŸ“ Save These Values

Create a file `aws-config.txt` in your project (âš ï¸ **DO NOT commit to git!**):

```bash

---

# Create config file (add to .gitignore)
cat > aws-config.txt << EOF
AWS_ACCOUNT_ID=123456789012
AWS_REGION=us-east-1
ECR_REPOSITORY_URI=123456789012.dkr.ecr.us-east-1.amazonaws.com/text-to-code
S3_BUCKET=text-to-code-models-priya
EC2_PUBLIC_IP=54.123.45.67
EC2_KEY_PAIR_PATH=~/Downloads/text-to-code-key.pem
EC2_KEY_PAIR_NAME=text-to-code-key
EOF


---

# Add to .gitignore
echo "aws-config.txt" >> .gitignore
```

**Replace the values with your actual values!**

---


---

## ðŸ› Troubleshooting


---

### Issue: "Access Denied" when running AWS CLI commands

**Solution:**

- Verify IAM user has correct policies attached
- Check AWS credentials: `aws configure list`
- Re-run `aws configure` with correct keys


---

### Issue: "BucketAlreadyExists" when creating S3 bucket

**Solution:**

- S3 bucket names are globally unique
- Try a different name with your initials or numbers


---

### Issue: Cannot SSH to EC2 instance

**Solution:**

- Wait 2-3 minutes after instance launch
- Verify security group allows SSH from your IP
- Check key file permissions: `chmod 400 key.pem`
- Verify instance is in "Running" state


---

### Issue: "Invalid key pair" error

**Solution:**

- Make sure you selected the correct key pair when launching instance
- Verify the .pem file matches the key pair name

---


---

## ðŸ’° Cost Estimate


---

### Free Tier (First 12 months):

- âœ… EC2 t2.micro: 750 hours/month free
- âœ… S3: 5 GB storage free
- âœ… ECR: 500 MB storage free
- âœ… Data transfer: 1 GB/month free


---

### After Free Tier (Estimated):

- EC2 t2.micro: ~$8-10/month (if running 24/7)
- S3 storage: ~$0.023/GB/month
- ECR storage: ~$0.10/GB/month
- Data transfer: ~$0.09/GB after first GB

**Estimated Monthly Cost**: $10-15/month (with minimal usage)

**ðŸ’¡ Tip**: Stop EC2 instance when not in use to save costs!

---


---

## ðŸŽ¯ Next Steps

Once all steps are complete and verified:

1. âœ… IAM user created and configured
2. âœ… ECR repository created
3. âœ… S3 bucket created
4. âœ… EC2 instance running
5. âœ… SSH connection tested

**Proceed to Phase 3: EC2 Deployment!**

See `IMPLEMENTATION_GUIDE.md` â†’ Phase 3 for deployment instructions.

---


---

## ðŸ“š Additional Resources

- [AWS IAM User Guide](https://docs.aws.amazon.com/IAM/latest/UserGuide/)
- [AWS ECR User Guide](https://docs.aws.amazon.com/ecr/)
- [AWS S3 User Guide](https://docs.aws.amazon.com/s3/)
- [AWS EC2 User Guide](https://docs.aws.amazon.com/ec2/)

---

**Phase 2 Status**: Ready to begin! ðŸš€

---

# Google Colab â†’ FastAPI Deployment Guide

Complete workflow for training in Colab and deploying to FastAPI.

---


---

## ðŸŽ¯ **Step-by-Step Process**


---

### **Step 1: Train Model in Google Colab**

Your notebook is already set up! Just run all cells.


---

#### In Colab:

```python

---

# Your notebook already has this code at the end (Cell 13)

---

# It saves the model automatically after training


---

# Check the save location (Cell 15)
args.save_dir  # Should be: "runs/saved_model/"


---

# After training completes, verify files:
!ls -lh runs/saved_model/
```

**Expected output:**
```
config.json
pytorch_model.bin
tokenizer_config.json
vocab.json
merges.txt
special_tokens_map.json
```

**Training Time**: ~30-40 minutes for 20 epochs

---


---

### **Step 2: Download Model from Colab to Your Mac**


---

#### **Option A: Download as ZIP (Recommended)**

Add this cell at the end of your Colab notebook:

```python

---

# Add this as a new cell in Colab
import shutil


---

# Create ZIP file
shutil.make_archive('codet5-finetuned', 'zip', 'runs/saved_model')


---

# Download the ZIP
from google.colab import files
files.download('codet5-finetuned.zip')
```

**Then on your Mac:**
```bash
cd /Users/priya/Data/Projects/TEXT2CODE


---

# Unzip the downloaded model
unzip ~/Downloads/codet5-finetuned.zip -d ./models/


---

# Verify
ls -lh ./models/
```


---

#### **Option B: Download via Google Drive**

Add this cell in Colab:

```python

---

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')


---

# Copy model to Drive
!cp -r runs/saved_model /content/drive/MyDrive/codet5-finetuned

print("âœ… Model saved to Google Drive: MyDrive/codet5-finetuned")
```

**Then on your Mac:**
1. Open Google Drive in browser
2. Download `codet5-finetuned` folder
3. Copy to FastAPI project:
```bash
cp -r ~/Downloads/codet5-finetuned/* /Users/priya/Data/Projects/TEXT2CODE/models/
```


---

#### **Option C: Direct S3 Upload from Colab**

Add this cell in Colab (after training):

```python

---

# Install AWS CLI in Colab
!pip install awscli -q


---

# Configure AWS credentials
import os
os.environ['AWS_ACCESS_KEY_ID'] = 'YOUR_ACCESS_KEY'
os.environ['AWS_SECRET_ACCESS_KEY'] = 'YOUR_SECRET_KEY'
os.environ['AWS_DEFAULT_REGION'] = 'us-east-1'


---

# Upload directly to S3
!aws s3 sync runs/saved_model/ s3://text-to-code-models-priya/models/v1/ \
  --exclude "*.git/*"

print("âœ… Model uploaded to S3!")
```

**Skip to Step 4 if you use this option!**

---


---

### **Step 3: Copy Model to FastAPI Project**

On your Mac:

```bash
cd /Users/priya/Data/Projects/TEXT2CODE


---

# Create models directory if it doesn't exist
mkdir -p models


---

# Copy downloaded model
cp -r ~/Downloads/codet5-finetuned/* ./models/

---

# OR (if unzipped differently)
cp -r ~/Downloads/runs/saved_model/* ./models/


---

# Verify all files are present
ls -lh ./models/


---

# Expected files:

---

# - config.json

---

# - pytorch_model.bin (or tf_model.h5)

---

# - tokenizer_config.json

---

# - vocab.json

---

# - merges.txt
```

---


---

### **Step 4: Update FastAPI Configuration**

Update `.env` file:

```bash
cd /Users/priya/Data/Projects/TEXT2CODE

cat > .env << EOF

---

# Use local model
MODEL_PATH=./models


---

# AWS Configuration (optional, for S3 storage)

---

# S3_BUCKET=text-to-code-models-priya

---

# S3_PREFIX=models/v1
EOF
```

---


---

### **Step 5: Test Locally (Optional but Recommended)**

```bash
cd /Users/priya/Data/Projects/TEXT2CODE


---

# Activate virtual environment
source venv/bin/activate


---

# Install dependencies (if not already installed)
pip install -r requirements.txt


---

# Run FastAPI locally
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

**Test in another terminal:**
```bash

---

# Health check
curl http://localhost:8000/health


---

# Generate code
curl -X POST http://localhost:8000/generate \
  -H "Content-Type: application/json" \
  -d '{"prompt": "Write a function to check if a string is palindrome", "max_length": 100}'
```

**Expected output**: Real Python code! ðŸŽ‰

---


---

### **Step 6: Upload Model to S3 (For EC2)**

This allows EC2 to auto-download the model on startup.

```bash
cd /Users/priya/Data/Projects/TEXT2CODE


---

# Make upload script executable (if not already)
chmod +x scripts/upload-model-to-s3.sh


---

# Upload model
./scripts/upload-model-to-s3.sh ./models/
```

**Verify upload:**
```bash
aws s3 ls s3://text-to-code-models-priya/models/v1/
```

---


---

### **Step 7: Build Docker Image**

**Make sure Docker Desktop is running!**

```bash
cd /Users/priya/Data/Projects/TEXT2CODE


---

# Option A: With local model (larger image, faster startup)

---

# Model will be baked into Docker image
docker build --platform linux/amd64 -t text-to-code:latest .


---

# Option B: Without local model (smaller image, downloads from S3)

---

# Remove ./models/ first so it's not included in image
mv models models-backup
docker build --platform linux/amd64 -t text-to-code:latest .
mv models-backup models
```

**Recommendation**: Use **Option B** (download from S3) for production

**Why?**
- âœ… Smaller Docker image (~300 MB vs ~1.5 GB)
- âœ… Easy to update model (just update S3, restart container)
- âš ï¸ Slower first startup (30 seconds to download)

---


---

### **Step 8: Push to ECR**

```bash

---

# Tag image
docker tag text-to-code:latest \
  518627289438.dkr.ecr.us-east-1.amazonaws.com/text-to-code:latest


---

# Login to ECR
aws ecr get-login-password --region us-east-1 | \
  docker login --username AWS --password-stdin \
  518627289438.dkr.ecr.us-east-1.amazonaws.com


---

# Push image
docker push 518627289438.dkr.ecr.us-east-1.amazonaws.com/text-to-code:latest
```

**Wait for upload** (~5-10 minutes depending on your internet speed)

---


---

### **Step 9: Deploy to EC2**

```bash

---

# SSH to EC2
ssh -i ~/Downloads/text-to-code-key.pem ubuntu@3.239.116.3


---

# Once on EC2:


---

# Stop and remove old container
docker stop text-to-code
docker rm text-to-code


---

# Pull new image
docker pull 518627289438.dkr.ecr.us-east-1.amazonaws.com/text-to-code:latest


---

# Run container
docker run -d \
  --name text-to-code \
  -p 80:8000 \
  --restart unless-stopped \
  -e AWS_DEFAULT_REGION=us-east-1 \
  -e S3_BUCKET=text-to-code-models-priya \
  -e S3_PREFIX=models/v1 \
  518627289438.dkr.ecr.us-east-1.amazonaws.com/text-to-code:latest


---

# Check logs (watch model download from S3)
docker logs -f text-to-code


---

# Verify it's running
docker ps


---

# Exit EC2
exit
```

---


---

### **Step 10: Test Production API**

```bash

---

# From your Mac


---

# Generate code (your fine-tuned model!)
curl -X POST http://3.239.116.3/generate \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "Write a function to find the sum of squares of n numbers",
    "max_length": 150,
    "temperature": 0.3
  }'
```

**Expected**: High-quality Python code from your fine-tuned model! ðŸš€

---


---

## ðŸŽ¯ **Quick Reference: Complete Workflow**

```bash

---

# 1. In Colab: Train model (run all cells)

---

# 2. In Colab: Download model as ZIP

---

# 3. On Mac:
cd /Users/priya/Data/Projects/TEXT2CODE
unzip ~/Downloads/codet5-finetuned.zip -d ./models/


---

# 4. Upload to S3
./scripts/upload-model-to-s3.sh ./models/


---

# 5. Build Docker (without local model for smaller image)
mv models models-backup
docker build --platform linux/amd64 -t text-to-code:latest .
mv models-backup models


---

# 6. Push to ECR
docker tag text-to-code:latest 518627289438.dkr.ecr.us-east-1.amazonaws.com/text-to-code:latest
aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin 518627289438.dkr.ecr.us-east-1.amazonaws.com
docker push 518627289438.dkr.ecr.us-east-1.amazonaws.com/text-to-code:latest


---

# 7. Deploy to EC2
ssh -i ~/Downloads/text-to-code-key.pem ubuntu@3.239.116.3
docker stop text-to-code && docker rm text-to-code
aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin 518627289438.dkr.ecr.us-east-1.amazonaws.com
docker pull 518627289438.dkr.ecr.us-east-1.amazonaws.com/text-to-code:latest
docker run -d --name text-to-code -p 80:8000 --restart unless-stopped -e AWS_DEFAULT_REGION=us-east-1 -e S3_BUCKET=text-to-code-models-priya -e S3_PREFIX=models/v1 518627289438.dkr.ecr.us-east-1.amazonaws.com/text-to-code:latest
exit


---

# 8. Test
curl http://3.239.116.3/health
```

---


---

## ðŸš¨ **Common Issues & Solutions**


---

### Issue 1: "Model files not found"

**Solution:**
```bash

---

# Check if files are in the right place
ls -lh ./models/


---

# Must have config.json!
cat ./models/config.json
```


---

### Issue 2: "Out of memory on EC2"

**Solution:**
- Use S3 download method (smaller Docker image)
- Or upgrade EC2 from t3.micro â†’ t3.small


---

### Issue 3: "Docker push taking forever"

**Solution:**
- Don't include model in Docker image (use S3 download)
- Model size: ~890 MB (adds 10 min to push time)


---

### Issue 4: "Model generating bad code"

**Possible causes:**
1. Wrong tokenizer (check: should be `RobertaTokenizer`)
2. Wrong prefix (check: should be `"Generate Python: "`)
3. Model not loaded (check logs: `docker logs text-to-code`)

---


---

## ðŸ“Š **Training Options in Colab**


---

### Quick Test (5 epochs, ~10 minutes):
```python
args.epochs = 5
```
Good for testing the pipeline, lower quality output.


---

### Balanced (10 epochs, ~20 minutes):
```python
args.epochs = 10
```
Good quality for demo purposes.


---

### Best Quality (20 epochs, ~40 minutes):
```python
args.epochs = 20  # Default in notebook
```
Production-ready quality.


---

### Fine-tune existing CodeT5:
```python

---

# In Cell 15, change from scratch to fine-tune
args.model_name_or_path = 'Salesforce/codet5-base'  # Already set!
```

---


---

## ðŸŽ **Bonus: One-Command Download from Colab**

Add this cell at the very end of your Colab notebook:

```python

---

# Combined: ZIP + Auto-download
import shutil
from google.colab import files

print("ðŸ“¦ Creating model archive...")
shutil.make_archive('codet5-trained', 'zip', 'runs/saved_model')

print("â¬‡ï¸ Starting download...")
print("Model will be saved to your Mac's Downloads folder")
print("Then run:")
print("  cd /Users/priya/Data/Projects/TEXT2CODE")
print("  unzip ~/Downloads/codet5-trained.zip -d ./models/")
print("  ./scripts/upload-model-to-s3.sh ./models/")

files.download('codet5-trained.zip')
print("âœ… Download complete!")
```

---


---

## âœ… **Checklist**

- [ ] Train model in Colab (all cells run successfully)
- [ ] Download model ZIP from Colab
- [ ] Extract to `./models/` on Mac
- [ ] Upload model to S3
- [ ] Build Docker image
- [ ] Push to ECR
- [ ] Deploy to EC2
- [ ] Test API endpoints
- [ ] Celebrate! ðŸŽ‰

---

**Need help?** Check:
- `MODEL_MANAGEMENT_GUIDE.md` - Model storage options
- `NOTEBOOK_VS_FASTAPI_COMPARISON.md` - Why CodeT5 > T5
- `PHASE3_COMPLETE.md` - EC2 deployment details

**Next step**: Train your model in Colab! ðŸš€


---

# IMPLEMENTATION CHECKLIST & STEP-BY-STEP GUIDE


---

## ðŸŽ¯ IMPLEMENTATION ORDER

Follow this exact order for smooth implementation:


---

### PHASE 1: LOCAL SETUP âœ… COMPLETE (2 hours)


---

#### Step 1.1: Repository Setup âœ…

- [X] Copy all files to your GitHub repository
- [X] Update README.md with your information
- [X] Replace YOUR_USERNAME, YOUR_ACCOUNT_ID placeholders
- [X] Commit and push to GitHub


---

#### Step 1.2: Local Testing âœ…

- [X] Create virtual environment: `python -m venv venv`
- [X] Activate environment: `source venv/bin/activate`
- [X] Install dependencies: `pip install -r requirements.txt`
- [X] Create .env from .env.example
- [X] Test API locally: `uvicorn app.main:app --reload`
- [X] Access http://localhost:8000/docs
- [X] Test health endpoint: `curl http://localhost:8000/health`


---

#### Step 1.3: Docker Testing âœ…

- [X] Build Docker image: `docker build -t text-to-code:latest .` âœ…
- [X] Run container: `docker run -p 8000:8000 text-to-code:latest` âœ…
- [X] Verify health: `curl http://localhost:8000/health` âœ…
- [X] Test code generation endpoint âœ…
- [X] Check logs: `docker logs <container-id>` âœ…

**ðŸ“‹ See `PHASE1_COMPLETE.md` for summary**


---

### PHASE 2: AWS SETUP (1-2 hours)

**ðŸ“– See `AWS_SETUP_DETAILED.md` for comprehensive step-by-step guide with screenshots descriptions**


---

#### Step 2.1: AWS Account Setup

- [X] Create/login to AWS account
- [X] Create IAM user with programmatic access
- [X] Attach policies: AmazonEC2FullAccess, AmazonS3FullAccess, AmazonECRFullAccess
- [X] Save access key ID and secret access key âš ï¸
- [X] Install AWS CLI: `pip install awscli` or `brew install awscli`
- [ ] Configure: `aws configure`
- [ ] Verify: `aws sts get-caller-identity`


---

#### Step 2.2: Create ECR Repository

```bash
aws ecr create-repository --repository-name text-to-code --region us-east-1
```

- [ ] Save repository URI (format:
- [ ] 
- [ ] ACCOUNT_ID.dkr.ecr.us-east-1.amazonaws.com/text-to-code)
- [ ] Verify: `aws ecr describe-repositories --region us-east-1`


---

#### Step 2.3: Create S3 Bucket

```bash
aws s3 mb s3://text-to-code-models-YOUR_NAME --region us-east-1
```

- [ ] Bucket name must be globally unique
- [ ] Verify: `aws s3 ls | grep text-to-code`
- [ ] Update .env with bucket name (optional)


---

#### Step 2.4: Launch EC2 Instance

- [ ] Go to EC2 Console â†’ Launch Instance
- [ ] Name: text-to-code-api
- [ ] AMI: Ubuntu Server 22.04 LTS
- [ ] Instance type: t2.micro (free tier) or t3.medium/large (production)
- [ ] Create new key pair: text-to-code-key (save .pem file securely!) âš ï¸
- [ ] Security group: text-to-code-sg with rules:
  - [ ] SSH (22) from My IP
  - [ ] HTTP (80) from anywhere
  - [ ] HTTPS (443) from anywhere
  - [ ] Custom TCP (8000) from anywhere
- [ ] Storage: 30 GB
- [ ] Launch instance
- [ ] Wait for "Running" state (1-2 minutes)
- [ ] Copy public IP address
- [ ] Test SSH: `ssh -i key.pem ubuntu@EC2_IP`


---

### PHASE 3: EC2 DEPLOYMENT (1 hour)


---

#### Step 3.1: Connect to EC2

```bash
chmod 400 your-key.pem
ssh -i your-key.pem ubuntu@YOUR_EC2_IP
```


---

#### Step 3.2: Run Setup Script

```bash

---

# Copy setup script to EC2
scp -i your-key.pem scripts/setup-ec2.sh ubuntu@YOUR_EC2_IP:~/


---

# SSH into EC2
ssh -i your-key.pem ubuntu@YOUR_EC2_IP


---

# Run setup
chmod +x setup-ec2.sh
./setup-ec2.sh


---

# Logout and login again for docker group
exit
ssh -i your-key.pem ubuntu@YOUR_EC2_IP
```


---

#### Step 3.3: Deploy Application

```bash

---

# Pull image (after you push it)
docker pull YOUR_ACCOUNT_ID.dkr.ecr.us-east-1.amazonaws.com/text-to-code:latest


---

#### Step 3.4: Test Public Access

```bash

---

# From your local machine
curl http://YOUR_EC2_IP/health
curl http://YOUR_EC2_IP/docs
```


---

### PHASE 4: CI/CD SETUP (30 minutes)


---

#### Step 4.1: GitHub Secrets

Go to GitHub repo â†’ Settings â†’ Secrets and variables â†’ Actions

Add these secrets:

- [ ] AWS_ACCESS_KEY_ID: Your IAM access key
- [ ] AWS_SECRET_ACCESS_KEY: Your IAM secret key
- [ ] AWS_ACCOUNT_ID: Your AWS account ID (12 digits)
- [ ] AWS_REGION: us-east-1
- [ ] EC2_HOST: Your EC2 public IP
- [ ] EC2_USERNAME: ubuntu
- [ ] EC2_SSH_KEY: Content of your .pem file
- [ ] S3_BUCKET: Your S3 bucket name
- [ ] S3_PREFIX: models/v1


---

#### Step 4.2: Test CI/CD

- [ ] Make a small change to README
- [ ] Commit and push to main branch
- [ ] Go to Actions tab in GitHub
- [ ] Watch the workflow run
- [ ] Verify deployment completes successfully


---

### PHASE 5: MONITORING (30 minutes)


---

#### Step 5.1: CloudWatch Setup

- [ ] Go to CloudWatch console
- [ ] Create alarm for EC2 CPU > 80%
- [ ] Create alarm for Memory > 85%
- [ ] Set up email notifications


---

#### Step 5.2: Test Monitoring

- [ ] Access metrics: http://YOUR_EC2_IP/metrics
- [ ] Generate some requests
- [ ] Check CloudWatch logs
- [ ] Verify metrics are being collected


---

### PHASE 6: DOCUMENTATION UPDATE (30 minutes)


---

#### Step 6.1: Update README

- [ ] Add your live demo URL
- [ ] Add screenshots of working API
- [ ] Update architecture diagram with your URLs
- [ ] Add your contact information


---

#### Step 6.2: Create Architecture Diagram

Use draw.io or similar to create:

- [ ] System architecture
- [ ] CI/CD pipeline flow
- [ ] AWS infrastructure diagram


---

## ðŸ“ PRE-INTERVIEW VERIFICATION CHECKLIST

Do this 24 hours before interview:

- [ ] API is accessible at http://YOUR_EC2_IP/
- [ ] API docs work at http://YOUR_EC2_IP/docs
- [ ] Health check returns healthy
- [ ] Can generate code successfully
- [ ] GitHub Actions shows green checkmark
- [ ] README has live demo link
- [ ] Can explain each component confidently


---

## ðŸŽ¤ INTERVIEW TALKING POINTS


---

### When discussing your project:

1. **Start with the problem**: "I built a text-to-code generation system to help developers write code faster"
2. **Highlight the ML**: "I fine-tuned Google's T5 model on 2,000+ coding tasks and achieved 86% accuracy"
3. **Emphasize MLOps**: "To make it production-ready, I implemented several MLOps best practices:"

   - Containerized with Docker for reproducibility
   - Set up CI/CD with GitHub Actions for automated deployment
   - Deployed on AWS EC2 with model weights in S3
   - Added monitoring with Prometheus metrics
   - Implemented health checks and logging
4. **Show results**: "You can try it live at [URL] - it generates Python code in under 500ms"
5. **Discuss challenges**: "Key challenges were optimizing model inference time and managing the large model in containers"
6. **Talk about improvements**: "I'm currently implementing [choose from future work] to make it even better"


---

### Demo Script:

```bash

---

# Show them the live API
curl -X POST http://YOUR_EC2_IP/generate \
  -H "Content-Type: application/json" \
  -d '{"prompt": "create a function to check if a number is prime"}'


---

# Show health check
curl http://YOUR_EC2_IP/health


---

# Show metrics
curl http://YOUR_EC2_IP/metrics
```


---

## ðŸ› TROUBLESHOOTING


---

### Container won't start

```bash

---

# Check logs
docker logs text-to-code


---

# Common issues:

---

# - Port already in use: sudo lsof -i :8000

---

# - Permission denied: sudo usermod -aG docker $USER

---

# - Out of memory: Upgrade instance type
```


---

### Health check fails

```bash

---

# Check if app is running
docker ps


---

# Check application logs
docker exec text-to-code cat /var/log/app.log


---

# Test internally
docker exec text-to-code curl http://localhost:8000/health
```


---

### ECR push fails

```bash

---

# Re-login to ECR
aws ecr get-login-password --region us-east-1 | \
  docker login --username AWS --password-stdin \
  YOUR_ACCOUNT_ID.dkr.ecr.us-east-1.amazonaws.com


---

# Verify credentials
aws sts get-caller-identity
```


---

### GitHub Actions fails

- Check secrets are set correctly
- Verify EC2 security group allows SSH from GitHub IPs
- Check EC2 instance is running
- Review workflow logs in Actions tab


---

## âš¡ QUICK COMMANDS REFERENCE


---

### Docker

```bash

---

# Build
docker build -t text-to-code:latest .


---

# Run
docker run -d -p 8000:8000 --name text-to-code text-to-code:latest


---

# Logs
docker logs -f text-to-code


---

# Stop
docker stop text-to-code && docker rm text-to-code


---

# Clean up
docker system prune -a
```


---

### AWS

```bash

---

# ECR login
aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin ACCOUNT.dkr.ecr.us-east-1.amazonaws.com


---

# List EC2 instances
aws ec2 describe-instances --query 'Reservations[*].Instances[*].[InstanceId,State.Name,PublicIpAddress]' --output table
```


---

### Testing

```bash

---

# Local test
curl http://localhost:8000/health


---

# EC2 test
curl http://YOUR_EC2_IP/health


---

# Generate code test
curl -X POST http://YOUR_EC2_IP/generate \
  -H "Content-Type: application/json" \
  -d '{"prompt": "create a function to reverse a string"}'
```


---

## ðŸŽ“ LEARNING RESOURCES

If you get stuck, refer to:

- FastAPI docs: https://fastapi.tiangolo.com/
- Docker docs: https://docs.docker.com/
- AWS EC2 guide: https://docs.aws.amazon.com/ec2/
- GitHub Actions: https://docs.github.com/en/actions
- Prometheus: https://prometheus.io/docs/


---

## ðŸ“ž GETTING HELP

If something doesn't work:

1. Check the error message carefully
2. Look in this troubleshooting section
3. Search the error on Stack Overflow
4. Check GitHub Issues of relevant tools
5. Ask in relevant Discord/Slack communities


---

## âœ… FINAL CHECKLIST

Before considering implementation complete:

- [ ] All files copied to repository
- [ ] Local testing successful
- [ ] Docker image builds successfully
- [ ] AWS resources created
- [ ] Application deployed on EC2
- [ ] Public URL accessible
- [ ] CI/CD pipeline working
- [ ] Monitoring configured
- [ ] README updated with live demo
- [ ] Can demo the project confidently
- [ ] Can explain each technology choice
- [ ] Have screenshots ready
- [ ] Practice interview questions


---

## ðŸŽ¯ SUCCESS CRITERIA

You know you're ready when you can:

âœ… Show working live demo
âœ… Explain architecture diagram
âœ… Discuss technology choices
âœ… Demo CI/CD pipeline
âœ… Show monitoring metrics
âœ… Explain deployment process
âœ… Answer "how would you improve this?"
âœ… Discuss challenges you faced

---

**Estimated Total Time**: 6-8 hours
**Recommended Schedule**: Spread over 2 days as per the crash course

Good luck! ðŸš€

---

# Model Management Guide

Complete guide for saving, loading, and deploying models with FastAPI.

---


---

## ðŸ“‹ Table of Contents

1. [Save Model from Notebook](#1-save-model-from-notebook)
2. [Download Pre-trained CodeT5](#2-download-pre-trained-codet5)
3. [Upload Model to S3](#3-upload-model-to-s3)
4. [Use Model in FastAPI](#4-use-model-in-fastapi)
5. [Model Storage Comparison](#5-model-storage-comparison)

---


---

## 1. Save Model from Notebook


---

### From Your Jupyter Notebook (After Training)

Your notebook already saves the model automatically at the end of training:

```python

---

# This code is in Cell 13 of your notebook
logger.info(f" Saving model in {args.save_dir}")
trainer.model.save_pretrained(args.save_dir)  # Saves model weights
tokenizer.save_pretrained(args.save_dir)      # Saves tokenizer
```

**Default save location**: `runs/saved_model/`


---

### Files Saved:
```
runs/saved_model/
â”œâ”€â”€ config.json              # Model architecture config
â”œâ”€â”€ pytorch_model.bin        # PyTorch weights (~890 MB for CodeT5-base)
â”œâ”€â”€ tf_model.h5             # TensorFlow weights (if TF model)
â”œâ”€â”€ tokenizer_config.json   # Tokenizer configuration
â”œâ”€â”€ vocab.json              # Vocabulary (32,000 tokens)
â”œâ”€â”€ merges.txt              # Byte-pair encoding merges
â””â”€â”€ special_tokens_map.json # Special tokens
```


---

### Copy to FastAPI Project:
```bash

---

# Copy from notebook location to FastAPI project
cp -r /path/to/notebook/runs/saved_model/* ./models/


---

# Or specify exact path (example)
cp -r ~/Documents/notebooks/runs/saved_model/* /Users/priya/Data/Projects/TEXT2CODE/models/
```

---


---

## 2. Download Pre-trained CodeT5

If you want to use CodeT5-base without training (good for testing):


---

### Method A: Using Python Script

```bash
cd /Users/priya/Data/Projects/TEXT2CODE


---

# Download and save CodeT5-base
python3 scripts/download-codet5.py
```

**Saves to**: `./models/codet5-base/`


---

### Method B: Manual Python Code

```python
from transformers import T5ForConditionalGeneration, RobertaTokenizer


---

# Download
model = T5ForConditionalGeneration.from_pretrained("Salesforce/codet5-base")
tokenizer = RobertaTokenizer.from_pretrained("Salesforce/codet5-base")


---

# Save
model.save_pretrained("./models/codet5-base")
tokenizer.save_pretrained("./models/codet5-base")
```


---

### Method C: Using Hugging Face CLI

```bash

---

# Install CLI
pip install huggingface_hub


---

# Download model
huggingface-cli download Salesforce/codet5-base \
  --local-dir ./models/codet5-base \
  --local-dir-use-symlinks False
```

---


---

## 3. Upload Model to S3

Upload your model to S3 so EC2 can automatically download it on startup.


---

### Quick Upload:

```bash

---

# Upload fine-tuned model from notebook
./scripts/upload-model-to-s3.sh ./runs/saved_model


---

# Or upload CodeT5-base
./scripts/upload-model-to-s3.sh ./models/codet5-base


---

# Or specify custom path
./scripts/upload-model-to-s3.sh /path/to/your/model
```


---

### Manual Upload (AWS CLI):

```bash

---

# Set variables
MODEL_DIR="./models/codet5-base"
S3_BUCKET="text-to-code-models-priya"
S3_PREFIX="models/v1"


---

# Upload
aws s3 sync "$MODEL_DIR/" "s3://$S3_BUCKET/$S3_PREFIX/" \
  --region us-east-1 \
  --exclude "*.git/*"
```


---

### Verify Upload:

```bash

---

# List files in S3
aws s3 ls s3://text-to-code-models-priya/models/v1/


---

# Check total size
aws s3 ls s3://text-to-code-models-priya/models/v1/ \
  --recursive --human-readable --summarize
```

---


---

## 4. Use Model in FastAPI

FastAPI supports 3 ways to load models:


---

### Option 1: Local Model (Fastest)

**Setup:**
```bash

---

# Copy model to ./models/
cp -r /path/to/saved_model/* ./models/


---

# Update .env
echo "MODEL_PATH=./models" > .env
```

**How it works:**
- FastAPI checks `./models/` for `config.json`
- If found, loads model from disk
- âœ… **Fastest** (no download)
- âŒ Requires model in Docker image (large image size)


---

### Option 2: S3 Model (Best for EC2)

**Setup:**
```bash

---

# Upload to S3 (see section 3)
./scripts/upload-model-to-s3.sh ./models/codet5-base


---

### Option 3: Hugging Face (Fallback)

**Setup:**
```bash

---

# No local model, no S3 configured

---

# .env is minimal or doesn't exist
```

**How it works:**
- FastAPI checks `./models/` (no config.json)
- Falls back to `Salesforce/codet5-base` from Hugging Face
- Downloads on first run, caches locally
- âœ… Zero setup
- âŒ Slower first startup
- âŒ Not your fine-tuned model

---


---

## 5. Model Storage Comparison

| Method | Setup Effort | Docker Image Size | Startup Time | Model Updates | Best For |
|--------|--------------|-------------------|--------------|---------------|----------|
| **Local** | Medium | ðŸ”´ Large (~1.5 GB) | ðŸŸ¢ Fast (5s) | ðŸ”´ Rebuild image | Development |
| **S3** | Medium | ðŸŸ¢ Small (~300 MB) | ðŸŸ¡ Medium (30s first time) | ðŸŸ¢ Update S3 only | Production (EC2) |
| **HuggingFace** | ðŸŸ¢ None | ðŸŸ¢ Small (~300 MB) | ðŸŸ¡ Medium (30s first time) | ðŸ”´ Can't customize | Quick testing |

---


---

## ðŸŽ¯ Recommended Workflow


---

### For Development (Local Mac):
```bash

---

# 1. Download CodeT5 for testing
python3 scripts/download-codet5.py


---

# 2. Update .env
echo "MODEL_PATH=./models/codet5-base" > .env


---

# 3. Run locally
source venv/bin/activate
uvicorn app.main:app --reload
```


---

### For Production (EC2):


---

#### With Fine-Tuned Model:
```bash

---

# 1. Train in notebook (saves to runs/saved_model/)

---

# 2. Upload to S3
./scripts/upload-model-to-s3.sh /path/to/runs/saved_model


---

# 3. Update .env in Docker
cat > .env << EOF
MODEL_PATH=./models
S3_BUCKET=text-to-code-models-priya
S3_PREFIX=models/v1
EOF


---

# 4. Build and deploy
docker build --platform linux/amd64 -t text-to-code:latest .

---

# ... push to ECR and deploy to EC2
```


---

#### With CodeT5-Base (No Training):
```bash

---

# 1. Just update app/model.py to use CodeT5 (already done!)

---

# 2. Build and deploy - it will auto-download from HuggingFace
docker build --platform linux/amd64 -t text-to-code:latest .
```

---


---

## ðŸ” Verify Model Loading


---

### Check Logs:

**Local model:**
```
INFO - Loading model from local path: ./models
INFO - Model loaded successfully from ./models
```

**S3 model:**
```
INFO - Downloading model from S3: text-to-code-models-priya/models/v1
INFO - Downloading config.json to ./models/config.json
INFO - Downloading pytorch_model.bin to ./models/pytorch_model.bin
INFO - Model downloaded from S3 successfully
INFO - Loading model from local path: ./models
```

**HuggingFace fallback:**
```
WARNING - Local model not found at ./models, using CodeT5-base model
INFO - Loading CodeT5-base from Hugging Face...
INFO - Model loaded successfully
```


---

### Test API:
```bash
curl http://localhost:8000/health

---

# Should return: {"status":"healthy","model_loaded":true}
```

---


---

## ðŸ“ Model Size Reference

| Model | Download Size | Disk Size | Memory (GPU) | Memory (CPU) |
|-------|---------------|-----------|--------------|--------------|
| CodeT5-base | ~220 MB | ~890 MB | ~1.5 GB | ~2.5 GB |
| CodeT5-small | ~80 MB | ~310 MB | ~800 MB | ~1.2 GB |
| T5-base | ~220 MB | ~890 MB | ~1.5 GB | ~2.5 GB |

**EC2 t3.micro** (1 GB RAM): âŒ Too small (use t3.small: 2 GB RAM minimum)

---


---

## ðŸš¨ Troubleshooting


---

### Model Not Loading:

```bash

---

# Check if model files exist
ls -lh ./models/


---

# Should see config.json, pytorch_model.bin, etc.
```


---

### S3 Download Fails:

```bash

---

# Check AWS credentials
aws sts get-caller-identity


---

# Check S3 bucket access
aws s3 ls s3://text-to-code-models-priya/models/v1/


---

# Check IAM permissions (needs s3:GetObject, s3:ListBucket)
```


---

### Out of Memory:

```bash

---

# Use smaller model

---

# Or upgrade EC2 instance type

---

# Or add swap space (not recommended for production)
```

---


---

## ðŸ“Œ Quick Reference Commands

```bash

---

# Download CodeT5
python3 scripts/download-codet5.py


---

# Upload to S3
./scripts/upload-model-to-s3.sh ./models/codet5-base


---

# Test locally
source venv/bin/activate && uvicorn app.main:app --reload


---

# Build Docker
docker build --platform linux/amd64 -t text-to-code:latest .


---

# Check model in Docker
docker run --rm text-to-code:latest ls -lh /app/models/
```

---

**Next**: After saving your model, proceed with Docker build and deployment! ðŸš€


---

# Model Update Workflow

Complete guide for updating your deployed model with new versions.

---


---

## ðŸ”„ **Model Update Scenarios**


---

### Scenario 1: First-Time Training (Colab â†’ Production)

---

### Scenario 2: Model Improvement (Retrain with More Data)

---

### Scenario 3: Hot-Swap Model (Zero Downtime Update)

---


---

## ðŸ“‹ **Scenario 1: First-Time Training**


---

### **Step 1: Train in Colab**

Open your notebook in Colab and run all cells.

**Add this as the LAST cell** for easy download:

```python

---

# Add as final cell in your Colab notebook
import shutil
from google.colab import files

print("ðŸ“¦ Creating model archive...")
shutil.make_archive('codet5-trained', 'zip', 'runs/saved_model')

print("â¬‡ï¸ Starting download...")
files.download('codet5-trained.zip')

print("âœ… Model saved! Next steps on your Mac:")
print("  1. cd /Users/priya/Data/Projects/TEXT2CODE")
print("  2. unzip ~/Downloads/codet5-trained.zip -d ./models/")
print("  3. ./scripts/upload-model-to-s3.sh ./models/")
print("  4. ssh to EC2 and restart container")
```

**Training time**: ~40 minutes for 20 epochs

---


---

### **Step 2: Download & Extract on Mac**

```bash
cd /Users/priya/Data/Projects/TEXT2CODE


---

# Extract the model
unzip ~/Downloads/codet5-trained.zip -d ./models/


---

# Verify files
ls -lh ./models/

---

# Should see: config.json, pytorch_model.bin, vocab.json, etc.
```

---


---

### **Step 3: Upload to S3**

```bash

---

# Upload model to S3
./scripts/upload-model-to-s3.sh ./models/
```

**What happens:**
- Uploads model files to S3 bucket: `text-to-code-models-priya`
- Location: `s3://text-to-code-models-priya/models/v1/`

**Upload time**: ~2-3 minutes

---


---

### **Step 4: Deploy to EC2 (Restart Container)**

```bash

---

# Restart container (downloads model from S3 on startup)
docker restart text-to-code


---

# Watch logs to see model loading
docker logs -f text-to-code


---

# You should see:

---

# "Downloading model from S3..."

---

# "Model loaded successfully"


---

# Exit when done (Ctrl+C, then exit)
exit
```

**Restart time**: ~1 minute (including S3 download)

---


---

### **Step 5: Test Updated Model**

```bash

---

## ðŸ”„ **Scenario 2: Model Improvement (Retrain)**

When you want to improve your model (more epochs, different data, etc.):


---

### **Step 1: Train New Version in Colab**

```python

---

# In your Colab notebook, modify training args:
args.epochs = 30  # More epochs for better quality

---

# Or use different dataset, hyperparameters, etc.


---

# Run training...

---

# Download as before
```

---


---

### **Step 2: Version Your Models (Recommended)**

```bash
cd /Users/priya/Data/Projects/TEXT2CODE


---

# Extract with version number
unzip ~/Downloads/codet5-trained.zip -d ./models/v2/


---

# Keep old version as backup

---

# ./models/v1/ (old)

---

# ./models/v2/ (new)
```

---


---

### **Step 3: Upload New Version to S3**

**Option A: Replace Current Version**
```bash

---

# This overwrites models/v1/ in S3
./scripts/upload-model-to-s3.sh ./models/v2/
```

**Option B: Upload as New Version**
```bash

---

# Upload to different S3 prefix
aws s3 sync ./models/v2/ s3://text-to-code-models-priya/models/v2/ \
  --region us-east-1 \
  --exclude "*.git/*"
```

---


---

### **Step 4: Update Environment & Restart**

**If using new version path (v2):**

```bash

---

# Stop container
docker stop text-to-code
docker rm text-to-code


---

# Run with new model version
docker run -d \
  --name text-to-code \
  -p 80:8000 \
  --restart unless-stopped \
  -e AWS_DEFAULT_REGION=us-east-1 \
  -e S3_BUCKET=text-to-code-models-priya \
  -e S3_PREFIX=models/v2 \
  518627289438.dkr.ecr.us-east-1.amazonaws.com/text-to-code:latest


---

# Just restart - it will download the new model from S3
docker restart text-to-code
```

---


---

## âš¡ **Scenario 3: Zero-Downtime Update**

For production, you want to update without API downtime:


---

### **Method 1: Blue-Green Deployment**

```bash

---

# Start new container on different port
docker run -d \
  --name text-to-code-v2 \
  -p 8001:8000 \
  --restart unless-stopped \
  -e AWS_DEFAULT_REGION=us-east-1 \
  -e S3_BUCKET=text-to-code-models-priya \
  -e S3_PREFIX=models/v2 \
  518627289438.dkr.ecr.us-east-1.amazonaws.com/text-to-code:latest


---

# Wait for startup (watch logs)
docker logs -f text-to-code-v2

---

# Wait until you see "Application startup complete"


---

# Test new version
curl http://localhost:8001/health


---

# If good, swap ports:
docker stop text-to-code
docker rm text-to-code

docker stop text-to-code-v2
docker run -d \
  --name text-to-code \
  -p 80:8000 \
  --restart unless-stopped \
  -e AWS_DEFAULT_REGION=us-east-1 \
  -e S3_BUCKET=text-to-code-models-priya \
  -e S3_PREFIX=models/v2 \
  518627289438.dkr.ecr.us-east-1.amazonaws.com/text-to-code:latest

docker rm text-to-code-v2
exit
```

**Downtime**: ~5 seconds (during swap)

---


---

### **Method 2: Using Load Balancer (Advanced)**

Set up AWS Application Load Balancer (ALB) with multiple EC2 instances:
- Update one instance at a time
- ALB routes traffic to healthy instances
- **Zero downtime!**

*(See CI/CD guide for automated blue-green deployment)*

---


---

## ðŸ“Š **Model Version Management**


---

### **Recommended S3 Structure**

```
s3://text-to-code-models-priya/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ v1/              # Production
â”‚   â”‚   â”œâ”€â”€ config.json
â”‚   â”‚   â”œâ”€â”€ pytorch_model.bin
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ v2/              # Testing/Staging
â”‚   â”‚   â”œâ”€â”€ config.json
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ archive/         # Old versions
â”‚       â”œâ”€â”€ 2025-01-01/
â”‚       â””â”€â”€ 2025-02-15/
```

---


---

## ðŸ”„ **Quick Reference: Update Commands**

```bash

---

# ===== ON MAC =====


---

# 1. Train in Colab (40 min)

---

# 2. Download & extract
cd /Users/priya/Data/Projects/TEXT2CODE
unzip ~/Downloads/codet5-trained.zip -d ./models/


---

# 3. Upload to S3
./scripts/upload-model-to-s3.sh ./models/


---

# ===== ON EC2 =====


---

# 4. Restart container
ssh -i ~/Downloads/text-to-code-key.pem ubuntu@3.239.116.3
docker restart text-to-code
docker logs -f text-to-code  # Watch model loading
exit


---

# ===== TEST =====


---

# 5. Verify
curl http://3.239.116.3/health
curl -X POST http://3.239.116.3/generate \
  -H "Content-Type: application/json" \
  -d '{"prompt": "Write a function to reverse a string", "max_length": 100}'
```

---


---

## ðŸš¨ **Rollback Plan (If New Model Has Issues)**


---

### **Quick Rollback:**

```bash

---

# Update environment to use old version
docker stop text-to-code
docker rm text-to-code

docker run -d \
  --name text-to-code \
  -p 80:8000 \
  --restart unless-stopped \
  -e AWS_DEFAULT_REGION=us-east-1 \
  -e S3_BUCKET=text-to-code-models-priya \
  -e S3_PREFIX=models/v1 \
  518627289438.dkr.ecr.us-east-1.amazonaws.com/text-to-code:latest

exit
```

**Rollback time**: ~1 minute

---


---

## ðŸ“ **Update Checklist**

- [ ] Train new model in Colab
- [ ] Download model ZIP
- [ ] Extract to local folder
- [ ] Test locally (optional)
- [ ] Upload to S3 (with version number)
- [ ] Update EC2 container
- [ ] Test API endpoints
- [ ] Monitor performance
- [ ] Keep old version for rollback

---


---

## ðŸŽ¯ **Best Practices**

1. **Always version your models** (`v1`, `v2`, etc.)
2. **Keep previous version in S3** for quick rollback
3. **Test locally before deploying** (if possible)
4. **Monitor API after update** (check latency, error rate)
5. **Document changes** (what dataset, epochs, parameters)

---


---

## ðŸ”® **Advanced: Automated Updates (CI/CD)**

Once you set up CI/CD (Phase 4):

1. **Push model to S3** from Colab
2. **Trigger GitHub Action** (webhook or manual)
3. **GitHub Action automatically:**
   - Updates EC2 container
   - Runs health checks
   - Rolls back if issues detected

**Result**: One-click model updates! ðŸš€

*(See Phase 4 guide for setup)*

---

**Next**: Ready to train your first model? Or want to see what else is left in the project?


---

# Notebook vs FastAPI Comparison


---

## âœ… What Was Fixed in FastAPI


---

### 1. **Model Choice**
- **Notebook**: Uses `Salesforce/codet5-base` âœ…
- **FastAPI (OLD)**: Used `t5-base` âŒ (not trained for code)
- **FastAPI (NEW)**: Now uses `Salesforce/codet5-base` âœ…


---

### 2. **Tokenizer**
- **Notebook**: Uses `RobertaTokenizer` âœ… (CodeT5 uses this)
- **FastAPI (OLD)**: Used `T5Tokenizer` âŒ  
- **FastAPI (NEW)**: Now uses `RobertaTokenizer` âœ…


---

### 3. **Prompt Prefix**
- **Notebook**: `"Generate Python: "` âœ…
- **FastAPI (OLD)**: `"generate python code: "` âŒ
- **FastAPI (NEW)**: `"Generate Python: "` âœ… (exact match!)


---

### 4. **Generation Parameters**
| Parameter | Notebook | FastAPI (OLD) | FastAPI (NEW) |
|-----------|----------|---------------|---------------|
| `top_p` | 0.95 | 0.95 | 0.95 âœ… |
| `top_k` | 50 | âŒ Missing | 50 âœ… |
| `repetition_penalty` | 2.0 | 1.2 | 2.0 âœ… |
| `do_sample` | True (implied) | Conditional | True âœ… |
| `num_return_sequences` | 1 | âŒ Missing | 1 âœ… |

---


---

## ðŸ“Š Key Differences Explained


---

### Why CodeT5 > T5-base?
- **T5-base**: General-purpose text-to-text model (translation, summarization)
- **CodeT5**: Specifically trained on **code** with code-specific tasks
  - Pre-trained on CodeSearchNet (Ruby, JS, Go, Python, PHP, C, C#)
  - Uses identifier-aware training (understands variable names, function names)
  - Trained with code-specific tasks: Masked Identifier Prediction, Identifier Tagging


---

### Why RobertaTokenizer?
CodeT5 paper states:
> "We train a Byte-level BPE tokenizer... This tokenizer largely reduces the length of tokenized code sequence by 30%-45% and avoids encoding common code tokens like brackets ['{', '}'] into unknown tokens."

T5's default tokenizer **breaks on code**, encoding brackets as `<unk>` tokens!


---

### Why "Generate Python:" Prefix?
From T5 paper:
> "Task prefixes matter when (1) doing multi-task training (2) your task is similar or related to one of the supervised tasks used in pre-training."

CodeT5 was trained with specific prefixes for different tasks. Using the correct prefix helps the model understand the task.

---


---

## ðŸ”„ Changes Made to `app/model.py`


---

### 1. Added RobertaTokenizer Import
```python
from transformers import T5ForConditionalGeneration, T5Tokenizer, RobertaTokenizer
```


---

### 2. Updated Fallback Model Loading
```python

---

# OLD:
self.tokenizer = T5Tokenizer.from_pretrained("t5-base")


---

# NEW:
self.tokenizer = RobertaTokenizer.from_pretrained("Salesforce/codet5-base")
```


---

### 3. Updated Prompt Prefix
```python

---

### 4. Updated Generation Parameters
```python

---

# NEW parameters matching notebook:
outputs = self.model.generate(
    **inputs,
    max_length=max_length,
    top_p=0.95,           # Nucleus sampling
    top_k=50,             # Top-K sampling (NEW!)
    repetition_penalty=2.0,  # Higher penalty for repetition
    num_return_sequences=1,  # Single output
    do_sample=True,       # Enable sampling
    early_stopping=True
)
```

---


---

## ðŸš€ Next Steps to Use Your Trained Model


---

### Option 1: Use Fine-Tuned Model from Notebook

If you trained a model using the notebook and saved it:

1. **Copy trained model** to your project:
   ```bash
   cp -r /path/to/notebook/runs/saved_model/* ./models/
   ```

2. **Update `.env`**:
   ```bash
   MODEL_PATH=./models
   ```

3. The FastAPI will automatically load your fine-tuned model!


---

### Option 2: Use CodeT5-base (Current Setup)

The FastAPI now uses `Salesforce/codet5-base` by default:
- âœ… No training needed
- âœ… Better than T5-base for code
- âŒ Not fine-tuned on MBPP (lower quality than your trained model)


---

### Option 3: Fine-Tune and Upload to S3

1. Train model using notebook
2. Upload to S3:
   ```bash
   aws s3 sync ./runs/saved_model/ s3://text-to-code-models-priya/models/v1/
   ```
3. FastAPI will auto-download from S3 on startup!

---


---

## ðŸ“ Testing the Improvements


---

### Before (with T5-base):
```bash
curl -X POST http://3.239.116.3/generate \
  -H "Content-Type: application/json" \
  -d '{"prompt": "def add_two_numbers", "max_length": 50}'
```
**Output**: `"False"` âŒ (nonsense)


---

### After (with CodeT5-base):
```bash
curl -X POST http://3.239.116.3/generate \
  -H "Content-Type: application/json" \
  -d '{"prompt": "Write a function to add two numbers", "max_length": 100}'
```
**Expected Output**: Actual Python code! âœ…

---


---

## ðŸŽ¯ Summary

| Issue | Root Cause | Fix |
|-------|-----------|-----|
| Returns "False" | T5-base has no code knowledge | Use CodeT5-base |
| Generates garbage | Wrong tokenizer breaks on code | Use RobertaTokenizer |
| Poor quality | Wrong prompt prefix | Use "Generate Python:" |
| Repetitive code | Low repetition penalty | Increase to 2.0 |
| Missing top_k | Parameter not set | Add top_k=50 |

**All issues fixed!** ðŸŽ‰

Next: Rebuild Docker and redeploy to test with real code generation!


---

# Current Project Status

**Last Updated**: December 31, 2025

---


---

## âœ… **Completed** (Phases 1-3)


---

### **Phase 1: Local Setup** âœ…
- Git repository initialized
- FastAPI application running
- Docker tested locally
- Virtual environment set up


---

### **Phase 2: AWS Setup** âœ…
- AWS Account configured
- ECR repository: `518627289438.dkr.ecr.us-east-1.amazonaws.com/text-to-code`
- S3 bucket: `text-to-code-models-priya`
- IAM policies configured


---

### **Phase 3: EC2 Deployment** âœ…
- EC2 instance running: `3.239.116.3`
- Docker container deployed
- API publicly accessible: `http://3.239.116.3`
- **Model**: CodeT5-base (not fine-tuned)


---

### **Phase 3.5: Model Integration** âœ…
- Fixed model from T5-base â†’ CodeT5-base
- Fixed tokenizer from T5Tokenizer â†’ RobertaTokenizer
- Fixed prompt prefix to "Generate Python:"
- Optimized generation parameters
- Created model management scripts

---


---

## âš ï¸ **Current Issue**

**Code generation quality is poor** because CodeT5-base is not fine-tuned.

**Current output**: Just echoes input or generates nonsense  
**Solution**: Train model in Colab (~40 minutes)

---


---

## ðŸš§ **Pending** (Phases 4-6)


---

### **Phase 4: CI/CD Setup** â³
- **Action needed**: Add GitHub Secrets
- **Time**: 30 minutes
- **Benefit**: Automated deployment on every code push


---

### **Phase 5: Monitoring** â³
- **Action needed**: Configure CloudWatch alarms
- **Time**: 20 minutes
- **Benefit**: Real-time alerts and dashboards


---

### **Phase 6: Model Training** â³
- **Action needed**: Run Colab notebook
- **Time**: 40 minutes
- **Benefit**: High-quality code generation!

---


---

## ðŸŽ¯ **Recommended Next Steps**


---

### **Priority 1: Train Model** ðŸ§  (40 min)
Get high-quality code generation working!

**Steps**:
1. Open notebook in Colab
2. Run all cells (train for 20 epochs)
3. Download model ZIP
4. Follow `MODEL_UPDATE_WORKFLOW.md`

**Result**: Production-quality code generation âœ¨

---


---

### **Priority 2: Fix S3 Permissions** ðŸ”’ (5 min)
Allow EC2 to download models from S3.

**Steps**:
1. AWS Console â†’ IAM â†’ Roles
2. Find `EC2-SSM-Role`
3. Add policy: `AmazonS3ReadOnlyAccess`

**Result**: EC2 can auto-download models from S3

---


---

### **Priority 3: Setup CI/CD** âš™ï¸ (30 min)
Automate deployments.

**Steps**:
1. Add 8 GitHub Secrets (see `WHATS_NEXT.md`)
2. Push code change
3. Watch automated deployment

**Result**: Zero-touch deployments

---


---

### **Priority 4: Setup Monitoring** ðŸ“Š (20 min)
Get visibility into API performance.

**Steps**:
1. Create CloudWatch alarms
2. Set up dashboards

**Result**: Alerts when issues occur

---


---

## ðŸ“š **Documentation Created**

All guides are in your project directory:


---

### **Getting Started:**
- `WHATS_NEXT.md` - Complete roadmap
- `CURRENT_STATUS.md` - This file
- `IMPLEMENTATION_GUIDE.md` - Master guide


---

### **Model Management:**
- `MODEL_UPDATE_WORKFLOW.md` - How to update models
- `MODEL_MANAGEMENT_GUIDE.md` - Storage options
- `COLAB_TO_FASTAPI_GUIDE.md` - Colab â†’ Production
- `NOTEBOOK_VS_FASTAPI_COMPARISON.md` - Technical details


---

### **Deployment:**
- `PHASE1_COMPLETE.md` - Local setup summary
- `PHASE3_COMPLETE.md` - EC2 deployment summary
- `AWS_SETUP_DETAILED.md` - AWS configuration


---

### **Scripts:**
- `scripts/download-codet5.py` - Download base model
- `scripts/upload-model-to-s3.sh` - Upload to S3
- `scripts/deploy-aws.sh` - AWS deployment
- `scripts/setup-ec2.sh` - EC2 configuration

---


---

## ðŸ“Š **Project Stats**

- **Time invested**: ~4 hours
- **Time remaining**: ~2 hours (for Phases 4-6)
- **Completion**: 60% (3 of 5 phases complete)
- **API Status**: âœ… Working (but needs better model)
- **Infrastructure**: âœ… Production-ready
- **Code Quality**: âœ… MLOps best practices

---


---

## ðŸš€ **Quick Commands**

```bash

---

# Test current API
curl http://3.239.116.3/health
curl -X POST http://3.239.116.3/generate \
  -H "Content-Type: application/json" \
  -d '{"prompt": "Write a function to sort a list", "max_length": 150}'


---

# Update model (after training)
cd /Users/priya/Data/Projects/TEXT2CODE
unzip ~/Downloads/codet5-trained.zip -d ./models/
./scripts/upload-model-to-s3.sh ./models/
ssh -i ~/Downloads/text-to-code-key.pem ubuntu@3.239.116.3
docker restart text-to-code
exit


---

# Check deployment
curl http://3.239.116.3/health
```

---


---

## ðŸ’¡ **Tips**

1. **Model training takes time** - Start it and take a break
2. **S3 permissions are important** - Fix them before uploading model
3. **Test locally first** - Catch issues early
4. **Version your models** - Easy rollback if needed
5. **Monitor after changes** - Check logs and metrics

---


---

## ðŸŽ‰ **What You've Accomplished**

âœ… Built a complete MLOps pipeline  
âœ… Deployed to AWS production  
âœ… Created professional documentation  
âœ… Implemented best practices  
âœ… Fixed critical model issues  
âœ… Created automation scripts  

**You're 60% done with a production-ready system!** ðŸš€

---


---

## ðŸ¤” **What Now?**

**Option 1**: Train model in Colab (best results)  
**Option 2**: Setup CI/CD (automation)  
**Option 3**: Take a break and come back fresh  

**I recommend**: Train the model first - it's the most impactful! Once you have high-quality code generation, the rest is "nice to have" automation.

**Let me know what you want to tackle next!** ðŸŽ¯


---

# âœ… PHASE 1 COMPLETE - SUMMARY


---

## ðŸŽ‰ All Local Setup Steps Completed!


---

### âœ… Step 1.1: Repository Setup
- All files in GitHub repository
- README.md updated with PriyaKPatel
- All placeholders replaced
- Git synced and pushed


---

### âœ… Step 1.2: Local Testing
- Virtual environment created (Python 3.9)
- All dependencies installed
- API running locally
- Health endpoint: âœ… Healthy
- Model loaded: âœ… T5-base from Hugging Face


---

### âœ… Step 1.3: Docker Testing
- Docker image built successfully âœ…
- Container running on port 8000 âœ…
- Health endpoint verified: `{"status":"healthy","model_loaded":true}` âœ…
- Code generation endpoint functional âœ…
- Model loaded in container: 222.90M parameters âœ…


---

## ðŸ“Š Current Status

**Local Development**: âœ… 100% Complete
- API accessible at http://localhost:8000
- Docker container running
- All endpoints functional

**Next Phase**: Phase 2 - AWS Setup (Requires manual steps)


---

## ðŸš€ Ready for Phase 2: AWS Setup

The following steps require your AWS account:

1. **AWS Account Setup** (15-30 min)
   - Create/login to AWS account
   - Create IAM user with programmatic access
   - Attach required policies

2. **Create AWS Resources** (15-30 min)
   - Create ECR repository
   - Create S3 bucket
   - Launch EC2 instance

3. **Deploy to EC2** (30-60 min)
   - Run setup script on EC2
   - Deploy Docker container
   - Test public access


---

## ðŸ“ Quick Reference


---

### Docker Commands
```bash

---

# Check running containers
docker ps


---

# View logs
docker logs <container-id>


---

# Test health
curl http://localhost:8000/health


---

# Test code generation
curl -X POST http://localhost:8000/generate \
  -H "Content-Type: application/json" \
  -d '{"prompt": "create a function", "max_length": 100}'
```


---

### Next Steps
1. Set up AWS account (if not already done)
2. Follow Phase 2 in IMPLEMENTATION_GUIDE.md
3. Configure GitHub Secrets for CI/CD
4. Deploy to EC2

---

**Phase 1 Status**: âœ… COMPLETE
**Ready for**: Phase 2 - AWS Setup


---

# Phase 3: EC2 Deployment - COMPLETE âœ…

**Date:** December 31, 2025  
**Status:** Successfully Deployed and Tested

---


---

## EC2 Instance Details

- **Public IP:** `3.239.116.3`
- **Instance Name:** `text-to-code-api`
- **Instance Type:** t3.micro
- **Region:** us-east-1
- **Key Pair:** `text-to-code-key.pem`

---


---

## Deployment Summary


---

### âœ… What Was Completed:

1. **EC2 Instance Launched**
   - Created new instance with proper network configuration
   - Auto-assign public IP enabled
   - Security groups configured for SSH (22), HTTP (80), and API (8000)

2. **Docker Container Deployed**
   - ECR Image: `518627289438.dkr.ecr.us-east-1.amazonaws.com/text-to-code:latest`
   - Container running on port 80
   - Auto-restart enabled (`--restart unless-stopped`)

3. **API Endpoints Tested**
   - âœ… `/health` - Healthy status confirmed
   - âœ… `/generate` - Code generation working (using T5-base model)
   - âœ… `/metrics` - Prometheus metrics available
   - âœ… `/` - Root endpoint accessible

---


---

## Test Results


---

### 1. Health Check
```bash
$ curl http://3.239.116.3/health
```
**Response:** âœ… Success (Status: healthy)


---

### 2. Code Generation
```bash
$ curl -X POST http://3.239.116.3/generate \
  -H "Content-Type: application/json" \
  -d '{"prompt": "create a function to calculate fibonacci numbers", "max_length": 150, "temperature": 0.7}'
```
**Response:**
```json
{
  "code": "...",
  "latency": 15.272,
  "timestamp": "2025-12-31T12:29:39.885532",
  "prompt": "create a function to calculate fibonacci numbers"
}
```
âœ… **Latency:** 15.27 seconds (first request with model loading)


---

### 3. Shorter Prompt Test
```bash
$ curl -X POST http://3.239.116.3/generate \
  -H "Content-Type: application/json" \
  -d '{"prompt": "def add_two_numbers", "max_length": 50, "temperature": 0.3}'
```
**Response:**
```json
{
  "code": "False",
  "latency": 0.643,
  "timestamp": "2025-12-31T12:30:12.974106",
  "prompt": "def add_two_numbers"
}
```
âœ… **Latency:** 0.64 seconds (much faster after warmup)


---

### 4. Prometheus Metrics
```bash
$ curl http://3.239.116.3/metrics
```
âœ… **Response:** Prometheus metrics available
- Python GC metrics
- Process metrics
- Request counters
- Latency histograms

---


---

## Public Access URLs

- **API Root:** http://3.239.116.3/
- **Health Check:** http://3.239.116.3/health
- **API Docs:** http://3.239.116.3/docs
- **Generate Endpoint:** http://3.239.116.3/generate (POST)
- **Metrics:** http://3.239.116.3/metrics
- **ReDoc:** http://3.239.116.3/redoc

---


---

## Notes

1. **Model:** Currently using **T5-base** from Hugging Face (not fine-tuned for code generation)
   - Generated code may not be accurate
   - Fine-tuning required for production-quality code generation

2. **Performance:**
   - First request: ~15 seconds (includes model loading)
   - Subsequent requests: < 1 second
   - Model loads on startup and stays in memory

3. **Security:**
   - SSH allowed from anywhere (0.0.0.0/0) - Should be restricted in production
   - HTTP/API ports open to public
   - No HTTPS configured yet

---


---

## Issues Resolved

1. **SSH Connectivity:** 
   - **Problem:** Original instance couldn't be reached via SSH
   - **Solution:** Relaunched instance with "Auto-assign public IP" enabled

2. **Disk Space:**
   - **Problem:** 8GB storage insufficient for Docker images
   - **Solution:** Relaunched with 20GB storage

3. **Docker Image Architecture:**
   - **Problem:** arm64 image incompatible with EC2 (amd64)
   - **Solution:** Built image with `--platform linux/amd64`

---


---

## Next Steps (Phase 4: CI/CD)

1. Add GitHub Secrets for automated deployment
2. Test GitHub Actions workflow
3. Enable automated deployments on push to main

See `IMPLEMENTATION_GUIDE.md` for Phase 4 details.


---

# ðŸ“¦ COMPLETE PROJECT FILES - WHAT YOU RECEIVED


---

## ðŸŽ‰ OVERVIEW

You've received a COMPLETE, PRODUCTION-READY MLOps project with 20+ files including:
- Full FastAPI application code
- Docker configuration
- Kubernetes manifests
- CI/CD pipeline
- AWS deployment scripts
- Comprehensive tests
- Monitoring setup
- Documentation


---

## ðŸ“ FILE STRUCTURE & PURPOSE


---

### ROOT LEVEL FILES


---

#### `README.md`
**Purpose**: Main project documentation
**Contains**:
- Project overview and features
- Quick start guide
- API usage examples
- Deployment instructions
- Architecture diagrams
**Action**: Update with your name, GitHub username, and live demo URL


---

#### `Dockerfile`
**Purpose**: Docker image definition
**Contains**:
- Multi-stage build for optimization
- Python 3.9 base image
- Health checks
- Uvicorn server configuration
**Action**: Use as-is, no changes needed


---

#### `docker-compose.yml`
**Purpose**: Local development environment
**Contains**:
- API service configuration
- Optional Prometheus and Grafana
- Volume mounts for development
- Network configuration
**Action**: Update S3_BUCKET and AWS credentials in environment


---

#### `requirements.txt`
**Purpose**: Python dependencies
**Contains**:
- FastAPI and Uvicorn
- PyTorch and Transformers
- AWS boto3
- Prometheus client
- Testing tools
**Action**: Use as-is, install with `pip install -r requirements.txt`


---

#### `.dockerignore`
**Purpose**: Exclude files from Docker build
**Contains**: Python cache, git files, tests, documentation
**Action**: Use as-is


---

#### `.gitignore`
**Purpose**: Exclude files from git
**Contains**: Python cache, virtual envs, models, secrets
**Action**: Use as-is


---

#### `.env.example`
**Purpose**: Environment variables template
**Contains**: AWS credentials, S3 bucket, configuration
**Action**: Copy to `.env` and fill in your values


---

#### `Makefile`
**Purpose**: Common command shortcuts
**Contains**: install, test, lint, build, deploy commands
**Action**: Use `make help` to see all commands


---

#### `IMPLEMENTATION_GUIDE.md`
**Purpose**: Step-by-step implementation guide
**Contains**:
- Phase-by-phase checklist
- Troubleshooting guide
- Interview talking points
- Quick command reference
**Action**: FOLLOW THIS GUIDE EXACTLY for implementation


---

### APP DIRECTORY (`app/`)


---

#### `app/__init__.py`
**Purpose**: Python package initialization
**Contains**: Version and metadata
**Action**: Update __author__ with your name


---

#### `app/main.py` â­ CORE FILE
**Purpose**: FastAPI application
**Contains**:
- API endpoints (/, /health, /generate, /metrics)
- Middleware (CORS, logging)
- Prometheus metrics
- Error handling
**Features**:
- Automatic model loading
- Request/response logging
- Health checks
- Metrics collection
**Action**: Use as-is, ready for production


---

#### `app/model.py` â­ CORE FILE
**Purpose**: T5 model wrapper
**Contains**:
- Model loading from S3 or local
- Code generation logic
- Post-processing
- Syntax validation
**Features**:
- Lazy loading
- S3 integration
- GPU/CPU support
- Error handling
**Action**: Use as-is, will load your fine-tuned model


---

#### `app/schemas.py`
**Purpose**: Pydantic models for validation
**Contains**:
- CodeRequest schema
- CodeResponse schema
- HealthResponse schema
- Error response schema
**Features**:
- Input validation
- Type checking
- API documentation
**Action**: Use as-is


---

### TESTS DIRECTORY (`tests/`)


---

#### `tests/__init__.py`
**Purpose**: Test package initialization
**Action**: Use as-is


---

#### `tests/test_api.py` â­ COMPREHENSIVE TESTS
**Purpose**: API endpoint tests
**Contains**:
- Health check tests
- Code generation tests
- Validation tests
- Integration tests
**Features**:
- 20+ test cases
- Coverage for all endpoints
- Error case testing
**Action**: Run with `pytest tests/`


---

### DEPLOYMENT DIRECTORY (`deployment/`)


---

#### `deployment/kubernetes/deployment.yaml`
**Purpose**: Kubernetes deployment manifest
**Contains**:
- Pod configuration (3 replicas)
- Resource limits
- Health probes
- HorizontalPodAutoscaler
**Features**:
- Auto-scaling (2-10 pods)
- Rolling updates
- Self-healing
**Action**: Replace YOUR_ACCOUNT_ID with your AWS account ID


---

#### `deployment/kubernetes/service.yaml`
**Purpose**: Kubernetes service manifest
**Contains**:
- LoadBalancer service
- ConfigMap for configuration
- Secrets template
**Features**:
- External access
- Configuration management
- Secret handling
**Action**: Update S3_BUCKET in secrets


---

#### `deployment/prometheus.yml`
**Purpose**: Prometheus configuration
**Contains**: Scraping configuration for API metrics
**Action**: Use as-is


---

### GITHUB WORKFLOWS (`.github/workflows/`)


---

#### `.github/workflows/deploy.yml` â­ CI/CD PIPELINE
**Purpose**: Automated deployment pipeline
**Contains**:
- Test job (pytest, linting, type checking)
- Build job (Docker build and push to ECR)
- Deploy job (EC2 deployment via SSH)
- Notification job
**Triggers**: Push to main branch
**Features**:
- Automated testing
- Docker image building
- ECR push
- EC2 deployment
- Health verification
**Action**: Add required secrets to GitHub repo


---

### SCRIPTS DIRECTORY (`scripts/`)


---

#### `scripts/setup-ec2.sh`
**Purpose**: EC2 instance setup
**Contains**:
- Docker installation
- AWS CLI installation
- CloudWatch agent setup
- Log rotation configuration
**Action**: Run on fresh EC2 instance


---

#### `scripts/deploy-local.sh`
**Purpose**: Local deployment
**Contains**:
- Docker build and run
- Health check verification
- Error handling
**Action**: Run after setting up .env


---

#### `scripts/deploy-aws.sh`
**Purpose**: AWS deployment
**Contains**:
- ECR repository creation
- Docker build and push
- S3 bucket creation
- Model upload
**Action**: Set AWS_ACCOUNT_ID environment variable before running


---

#### `scripts/setup-project.sh`
**Purpose**: Project initialization
**Contains**:
- Directory structure creation
- Git initialization
- User information collection
**Action**: Run first to set up project


---

### MODELS DIRECTORY (`models/`)


---

#### `models/.gitkeep`
**Purpose**: Ensure directory is tracked by git
**Action**: Place your fine-tuned T5 model here


---

## ðŸš€ HOW TO USE THESE FILES


---

### STEP 1: ORGANIZE FILES (5 minutes)

```bash

---

# Create project directory
mkdir text-to-code-generation
cd text-to-code-generation


---

# Copy ALL files maintaining structure:
text-to-code-generation/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ model.py
â”‚   â””â”€â”€ schemas.py
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ test_api.py
â”œâ”€â”€ deployment/
â”‚   â”œâ”€â”€ kubernetes/
â”‚   â”‚   â”œâ”€â”€ deployment.yaml
â”‚   â”‚   â””â”€â”€ service.yaml
â”‚   â””â”€â”€ prometheus.yml
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ setup-ec2.sh
â”‚   â”œâ”€â”€ deploy-local.sh
â”‚   â”œâ”€â”€ deploy-aws.sh
â”‚   â””â”€â”€ setup-project.sh
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ deploy.yml
â”œâ”€â”€ models/
â”‚   â””â”€â”€ .gitkeep
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .dockerignore
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .env.example
â”œâ”€â”€ Makefile
â”œâ”€â”€ README.md
â””â”€â”€ IMPLEMENTATION_GUIDE.md
```


---

### STEP 2: INITIAL SETUP (10 minutes)

```bash

---

# Make scripts executable
chmod +x scripts/*.sh


---

# Create virtual environment
python -m venv venv
source venv/bin/activate


---

# Install dependencies
pip install -r requirements.txt


---

# Create .env from template
cp .env.example .env

---

# Edit .env with your configuration


---

# Open http://localhost:8000/docs
```


---

### STEP 3: DOCKER TESTING (10 minutes)

```bash

---

# Build Docker image
docker build -t text-to-code:latest .


---

# Test in another terminal
curl http://localhost:8000/health
curl -X POST http://localhost:8000/generate \
  -H "Content-Type: application/json" \
  -d '{"prompt": "create a hello world function"}'
```


---

### STEP 4: AWS DEPLOYMENT (Follow IMPLEMENTATION_GUIDE.md)

See `IMPLEMENTATION_GUIDE.md` for detailed AWS setup and deployment.


---

### STEP 5: CI/CD SETUP (Follow IMPLEMENTATION_GUIDE.md)

See `IMPLEMENTATION_GUIDE.md` for GitHub Actions configuration.


---

## ðŸŽ¯ KEY FILES FOR INTERVIEW

When discussing your project in interview, reference these files:

1. **`app/main.py`** - "I built a FastAPI application with health checks, metrics, and proper error handling"

2. **`Dockerfile`** - "I used multi-stage builds to optimize the Docker image size"

3. **`.github/workflows/deploy.yml`** - "I implemented CI/CD with GitHub Actions that automatically tests, builds, and deploys"

4. **`app/model.py`** - "I created a model wrapper that handles loading from S3 and provides clean inference API"

5. **`deployment/kubernetes/`** - "I have Kubernetes manifests ready for container orchestration with auto-scaling"

6. **`tests/test_api.py`** - "I wrote comprehensive tests covering all endpoints and edge cases"


---

## ðŸ“Š WHAT MAKES THIS PRODUCTION-READY

âœ… **Clean Code**: Well-structured, documented, type-hinted
âœ… **Testing**: Comprehensive test suite with pytest
âœ… **Containerization**: Multi-stage Docker build
âœ… **CI/CD**: Automated pipeline with GitHub Actions
âœ… **Cloud Deployment**: AWS EC2 with ECR and S3
âœ… **Monitoring**: Prometheus metrics and health checks
âœ… **Documentation**: README, API docs, implementation guide
âœ… **Error Handling**: Proper exception handling and logging
âœ… **Scalability**: Kubernetes manifests with auto-scaling
âœ… **Security**: Secrets management, environment variables


---

## ðŸ”§ CUSTOMIZATION POINTS

You should customize these parts:


---

### Required Changes:
1. `README.md` - Replace YOUR_USERNAME, YOUR_ACCOUNT_ID, YOUR_NAME
2. `.github/workflows/deploy.yml` - Add GitHub secrets
3. `deployment/kubernetes/*.yaml` - Replace YOUR_ACCOUNT_ID
4. `app/__init__.py` - Update __author__


---

### Optional Enhancements:
1. Add your actual fine-tuned model to `models/`
2. Customize prompts in `app/model.py`
3. Add more test cases in `tests/`
4. Add authentication to API
5. Implement caching layer


---

## â±ï¸ TIME ESTIMATES

- **File Organization**: 5 minutes
- **Local Setup & Testing**: 30 minutes
- **Docker Testing**: 20 minutes
- **AWS Setup**: 1-2 hours
- **Deployment**: 1 hour
- **CI/CD Setup**: 30 minutes
- **Documentation Update**: 30 minutes
- **Total**: 4-6 hours


---

## ðŸŽ“ LEARNING FROM EACH FILE

Each file demonstrates important concepts:

- **`Dockerfile`**: Multi-stage builds, optimization
- **`docker-compose.yml`**: Service orchestration
- **`.github/workflows/`**: CI/CD automation
- **`app/main.py`**: API design, middleware
- **`app/model.py`**: ML model serving patterns
- **`tests/`**: Test-driven development
- **`deployment/kubernetes/`**: Container orchestration


---

## ðŸ’¡ PRO TIPS

1. **Don't change working code** - These files are production-ready
2. **Follow IMPLEMENTATION_GUIDE.md** - It has the exact order
3. **Test locally first** - Before deploying to cloud
4. **Take screenshots** - Document each working stage
5. **Practice demo** - Run through API calls before interview
6. **Understand, don't memorize** - Know why each part exists


---

## ðŸ†˜ IF YOU GET STUCK

1. Check `IMPLEMENTATION_GUIDE.md` troubleshooting section
2. Review error messages carefully
3. Check Docker logs: `docker logs <container>`
4. Verify environment variables in `.env`
5. Ensure AWS credentials are correct
6. Check GitHub Actions logs for CI/CD issues


---

## âœ… FINAL VERIFICATION

Before interview, verify:
- [ ] All files are in correct directories
- [ ] Local testing works
- [ ] Docker build succeeds
- [ ] API is deployed and accessible
- [ ] CI/CD pipeline is green
- [ ] Can demo live API
- [ ] README has your information
- [ ] Can explain each component


---

## ðŸ“ž QUICK REFERENCE

**Start local server**: `uvicorn app.main:app --reload`
**Build Docker**: `docker build -t text-to-code:latest .`
**Run tests**: `pytest tests/ -v`
**Deploy local**: `./scripts/deploy-local.sh`
**Deploy AWS**: `./scripts/deploy-aws.sh`
**Check health**: `curl http://localhost:8000/health`

---

**YOU NOW HAVE EVERYTHING YOU NEED!** ðŸš€

Just follow the IMPLEMENTATION_GUIDE.md step by step, and you'll have a fully working MLOps project deployed to AWS with CI/CD in 4-6 hours.

Good luck with your interview! ðŸ’ª